{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract, Transform, Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, target_table):\n",
    "    # Some custom-built Python logic to load data to SQL\n",
    "    data_frame.to_sql(name=target_table, con=POSTGRES_CONNECTION)\n",
    "    print(f\"Loading data to the {target_table} table\")\n",
    "\n",
    "# Now, run the data pipeline\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, target_table):\n",
    "    # Define a function named 'load' that takes two arguments:\n",
    "    # 'data_frame' - the data to be loaded,\n",
    "    # 'target_table' - the name of the target table in the SQL database where data will be loaded.\n",
    "\n",
    "    # Some custom-built Python logic to load data to SQL\n",
    "    data_frame.to_sql(name=target_table, con=POSTGRES_CONNECTION)\n",
    "    # Convert the 'data_frame' to a SQL table using the 'to_sql' method from the Pandas library.\n",
    "    # 'name' specifies the target table name in the SQL database.\n",
    "    # 'con' specifies the connection object to the Postgres database (assumed to be defined elsewhere as POSTGRES_CONNECTION).\n",
    "\n",
    "    print(f\"Loading data to the {target_table} table\")\n",
    "    # Print a message to the console indicating that data is being loaded into the specified target table.\n",
    "\n",
    "# Now, run the data pipeline\n",
    "# Execute the series of functions defined earlier to extract, transform, and load data.\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "# Call the 'extract' function with the argument 'file_name' set to \"raw_data.csv\".\n",
    "# The 'extract' function is assumed to read the raw data from this CSV file and return it as a DataFrame.\n",
    "# Store the returned DataFrame in the variable 'extracted_data'.\n",
    "\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "# Call the 'transform' function with the argument 'data_frame' set to the previously extracted data.\n",
    "# The 'transform' function is assumed to perform data cleaning and transformations.\n",
    "# Store the returned transformed DataFrame in the variable 'transformed_data'.\n",
    "\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n",
    "# Call the 'load' function with the argument 'data_frame' set to the transformed data,\n",
    "# and 'target_table' set to \"cleaned_data\".\n",
    "# This function will load the transformed data into the specified SQL table \"cleaned_data\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- This code snippet demonstrates an Extract, Transform, Load (ETL) process. \n",
    "- It first defines a load function that takes a DataFrame and target table name as input and loads the DataFrame into a PostgreSQL database table. \n",
    "- Then, it calls extract (not shown) to get data from a CSV file, transform (also not shown) to process that data, and finally load to put the transformed data into a new table. The POSTGRES_CONNECTION variable (not shown) would hold the connection details to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract, Load, Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(source_table, target_table):\n",
    "  \"\"\"\n",
    "  Transforms data from a source table to a target table using SQL.\n",
    "\n",
    "  Args:\n",
    "    source_table: The name of the source table.\n",
    "    target_table: The name of the target table.\n",
    "  \"\"\"\n",
    "  data_warehouse.run_sql(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "  SELECT\n",
    "    <field-name>, <field-name>, ...  -- List the fields to select\n",
    "  FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "# Similar to ETL pipelines, call the extract, load, and transform functions\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_data\")\n",
    "transform(source_table=\"raw_data\", target_table=\"cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(source_table, target_table):\n",
    "    \"\"\"\n",
    "    Transforms data from a source table to a target table using SQL.\n",
    "\n",
    "    Args:\n",
    "        source_table: The name of the source table.\n",
    "        target_table: The name of the target table.\n",
    "    \"\"\"\n",
    "    # Run an SQL query to create the target table by selecting data from the source table\n",
    "    data_warehouse.run_sql(f\"\"\"\n",
    "    CREATE TABLE {target_table} AS\n",
    "    SELECT\n",
    "        <field-name>, <field-name>, ...  -- List the fields to select\n",
    "    FROM {source_table};\n",
    "    \"\"\")\n",
    "    # The SQL query creates a new table (target_table) and populates it with selected fields from the source_table\n",
    "    # The fields to be selected need to be specified in the SQL query\n",
    "    # The function assumes that data_warehouse.run_sql executes the given SQL query\n",
    "\n",
    "# Similar to ETL pipelines, call the extract, load, and transform functions\n",
    "\n",
    "# Call the extract function to read raw data from a CSV file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "# The extracted data is stored in the variable extracted_data\n",
    "\n",
    "# Call the load function to load the extracted data into a raw_data table in the database\n",
    "load(data_frame=extracted_data, table_name=\"raw_data\")\n",
    "# The extracted data is loaded into the raw_data table in the SQL database\n",
    "\n",
    "# Call the transform function to transform data from the raw_data table to the cleaned_data table\n",
    "transform(source_table=\"raw_data\", target_table=\"cleaned_data\")\n",
    "# The transform function creates the cleaned_data table by selecting and processing data from the raw_data table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- This code snippet demonstrates a simplified ETL (Extract, Transform, Load) process. \n",
    "- The transform function uses SQL to create a new table (target_table) by selecting specific fields from an existing table (source_table). \n",
    "- The code then shows how to chain together extract (reading data from a CSV), load (putting data into a table), and transform (processing data within the database) operations, mimicking a typical ETL workflow. Note that <field-name> is a placeholder and needs to be replaced with the actual field names. \n",
    "- The data_warehouse and extract and load functions are assumed to be defined elsewhere."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
