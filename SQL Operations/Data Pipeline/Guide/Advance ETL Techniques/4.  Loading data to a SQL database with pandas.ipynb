{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading data to a SQL database with pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQL databases are one of the most common landing zones for data after transformation in a pipeline. \n",
    "- It's easy to connect a visualization tool directly to a SQL database, and most data analysts and scientists are comfortable using SQL to query data. \n",
    "- To load a DataFrame to a Postgres database, pandas provides the dot-to_sql method. \n",
    "- This method is called on the DataFrame to be loaded, and takes parameters name, con, if_exists, index, and index_label. \n",
    "- Let's take a closer look, with the help of an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides `.to_sql()` to persist data to SQL\n",
    "\n",
    "- `name`\n",
    "- `con`\n",
    "- `if_exists`\n",
    "- `index`\n",
    "- `index_label`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Persisting data to Postgres with pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to the read_sql function, a connection object needs to be created to load data to a SQL database. \n",
    "- This can be done in an identical way as before, by creating a connection URI and engine with the help of sqlalchemy. \n",
    "- This connection URI stores the engine, username, password, host, port, and database, which is the \"market\" database, in our example. \n",
    "- In our stock data example, dot-to_sql is called on the clean_stock_data DataFrame. \n",
    "- \"filtered_stock_data\" is passed to the name parameter, which is the name of the table in the market database that the DataFrame will be written to. \n",
    "- The recently-created db_engine is passed to the con parameter, forming a connection with the market database. \n",
    "- The value passed to if_exists determines how data is added to the table in a database. \n",
    "- If \"append\" is added, data is added to the existing table. \n",
    "- Passing \"replace\" overwrites the existing data in the table with the current DataFrame. \n",
    "- Here, data stored in the clean_stock_data DataFrame is appended to the existing \"filtered_stock_data\" table in the \"market\" database. \n",
    "- The index parameter takes a boolean value, True if the index is to be loaded to the SQL table, and False otherwise. \n",
    "- If index is set to True, the index_label parameter can be supplied. \n",
    "- This is the name of the column in SQL that the pandas index will be written to. \n",
    "- In our example, the index is written to Postgres, and has column name \"timestamps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection object\n",
    "connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/market\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .to_sql() method to persist data to SQL\n",
    "clean_stock_data.to_sql(\n",
    "    name=\"filtered_stock_data\",\n",
    "    con=db_engine,\n",
    "    if_exists=\"append\",\n",
    "    index=True,\n",
    "    index_label=\"timestamps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Validating data persistence with pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once data has been loaded to a Postgres database, it's important to validate that data has been persisted as expected. \n",
    "- If not, downstream consumers and processes will be limited by faulty data, and data users may even lose confidence in the data created by the pipeline. \n",
    "- First and foremost, data should be \"query-able\" and return logical results. \n",
    "- This includes making sure that counts match between transformed data and persisted data, and validating that each row is present in both DataFrames. \n",
    "- With pandas, this is easy! Using the read_sql function from earlier, the data loaded to a SQL database can be queried for validation. \n",
    "- This DataFrame can then be compared with the transformed DataFrame, to validate counts and record equality. \n",
    "- Including these manual data quality checks is important not only when developing a data pipeline, but also in monitoring and alerting efforts. \n",
    "- Providing this layer of validation helps instill trust in your data pipelines and ensures a robust solution has been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to validate that data is persisted as expected.\n",
    "- Ensure data can be queried\n",
    "- Make sure counts match\n",
    "- Validate that each row is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data written to SQL table\n",
    "to_validate = pd.read_sql(\"SELECT * FROM cleaned_stock_data\", db_engine)\n",
    "\n",
    "# Validate counts, record equality, etc\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
