{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Source systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Almost all data pipelines are initiated by extracting data from a source system. \n",
    "- In this course, we'll extract data from source systems such as CSV, parquet, and JSON files, as well as dynamic stores such as SQL databases. \n",
    "- Data can also be pulled from APIs, which are commonly used to ingest data from a third party. \n",
    "- Within organizations, data lakes and warehouses are popular source systems for data pipelines. \n",
    "- In more advanced pipelines, it's even common to web scrape the data. \n",
    "- Besides these, there are tons of other sources from which data can be pulled. \n",
    "- In this video, we'll explore pulling data from tabular source systems, including parquet files and SQL databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source systems\n",
    "\n",
    "In this course:\n",
    "\n",
    "    • CSV files\n",
    "    • Parquet files\n",
    "    • JSON files\n",
    "    • SQL databases\n",
    "\n",
    "Data is also sourced from:\n",
    "\n",
    "    • APIs\n",
    "    • Data lakes\n",
    "    • Data warehouses\n",
    "    • Web scraping\n",
    "    • ... and so many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading in parquet files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chances are, you've probably read a CSV file or two into memory using pandas. \n",
    "- However, many other tabular file types can be used to store data. \n",
    "- One such file type is Parquet. Apache Parquet is an open-source, column-oriented file format designed for efficient field storage and retrieval. \n",
    "- Working with parquet files in pandas is similar to working with CSV files, except reading and writing are much faster! \n",
    "- To read a parquet file into a DataFrame, first, import pandas as pd. \n",
    "- Then, pass the file path to the pd-dot-read_parquet function, and if you'd like, pass an engine, such as \"fastparquet\". \n",
    "- It's that easy to ingest a parquet file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in parquet files\n",
    "\n",
    "Parquet files:\n",
    "\n",
    "- Open source, column-oriented file format designed for efficient field storage and retrieval\n",
    "- Similar to working with CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the parquet file into memory\n",
    "raw_stock_data = pd.read_parquet(\"raw_stock_data.parquet\", engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Connecting to SQL databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQL databases are one of the most commonly used tabular data sources. \n",
    "- You can query data from a SQL database and store it in a DataFrame, using the read_sql function in pandas. \n",
    "- To use the read_sql function, a connection object to the SQL database must be created. \n",
    "- This can be done using sqlalchemy's create_engine function. create_engine takes a connection URI, which stands for \"uniform resource identifier\". \n",
    "- URIs allow connection information to be formatted in a string, which can then be passed to other tools. \n",
    "- These strings take the format: schema_identifier, username, password, host, port, db. We'll be using the postgresql-plus-psycopg2 schema identifier, and when needed, other connection details will be provided. \n",
    "- The connection object returned by the create_engine function should be passed to the read_sql function, along with a query. \n",
    "- This query can be as complex as you'd like, but should ultimately SELECT data from the database of interest. \n",
    "- The result of the query is stored in a DataFrame, and is available in memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to SQL databases\n",
    "\n",
    "- Data can be pulled from SQL databases into a `pandas` DataFrame\n",
    "- Requires a connection URI to build an engine, and connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "# Connection URI: schema_identifier://username:password@host:port/db\n",
    "connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/market\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Query the SQL database\n",
    "raw_stock_data = pd.read_sql(\"SELECT * FROM raw_stock_data LIMIT 10\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modularity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We've explored two common ways to extract data from tabular data sources using functionality such as read_parquet and read_sql. \n",
    " - When we write code to build a data pipeline, it's important to make sure that it's modular in nature. \n",
    " - Separating distinct logic into functions increases the readability within a pipeline. \n",
    " - When building data pipelines, it's a great idea to separate \"extract\", \"transform\" and \"load\" logic into separate functions. \n",
    " - Modularizing your code also adheres to the PEP-8 principle \"don't repeat yourself\", and makes code reusable! Above, we've reformatted our logic from the last slide into a function. \n",
    " - Now, this code can be reused, which helps save time when developing, and can even expedite troubleshooting. \n",
    " - As you build more data pipelines, modularizing your logic will become more natural, and even provides a framework when getting started on a new pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modularity**\n",
    "\n",
    "Separating logic into functions\n",
    "\n",
    "- Increases readability within a pipeline\n",
    "- Adheres to the principle \"don't repeat yourself\"\n",
    "- Expedites troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_sql(connection_uri, query):\n",
    "  \"\"\"\n",
    "  Create an engine, query data and return DataFrame\n",
    "  \"\"\"\n",
    "  db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "  return pd.read_sql(query, db_engine)\n",
    "\n",
    "extract_from_sql(\"postgresql+psycopg2://.../market\", \"SELECT ... LIMIT 10;\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
