{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Monitoring a data pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once a data pipeline is developed, it should be monitored for changes to data, and failures during execution. \n",
    "- Sometimes, source systems fail to provide data, or data types change. \n",
    "- Other times, the tools that Data Engineers had previously used become deprecated or functionality changes. \n",
    "- Whatever the reason, monitoring a data pipeline ensures the solution is transparent, and proper alerting notifies Data Engineers of an issue before data consumers discover it themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing data\n",
    "- Shifting data types\n",
    "- Package deprecation or functionality change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logging data pipeline performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this course, we'll use logging to alert engineers of data pipeline performance. \n",
    "- Logs are messages created and written during the execution of a pipeline. \n",
    "- They are configured by the developing party, and document the performance of a pipeline. \n",
    "- Logs provide a starting point when solutions fail by letting Data Engineers revisit the execution of the pipeline. \n",
    "- Logs are the foundation for all monitoring and alerting efforts, and are essential for creating transparent data pipelines. \n",
    "- The logging module in Python makes it easy to configure and create your own logs. \n",
    "- There are six levels of logging provided by the logging module. We'll explore four; debug, info, warning, and error. \n",
    "- Each has an associated function and are used to reflect differing severity events. \n",
    "- Debug logs are typically used when building a data pipeline, and give a Data Engineer insight into things such as data dimensionality, type, and variable values. \n",
    "- The info function is used to provide basic information and checkpoints throughout the execution of a pipeline, such as notifying an engineer about operations that occur on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document performance at execution\n",
    "- Provides a starting point when a solution fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "# Create different types of logs\n",
    "logging.debug(f\"Variable has value {path}\")\n",
    "logging.info(\"Data has been transformed and will now be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG: Variable has value raw_file.csv\n",
    "INFO: Data has been transformed and will now be Loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logging warnings and errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition to debug and info-level logs, warnings and errors should also be captured using logging. \n",
    "- Warnings are logged when something unexpected happens, but an exception has not necessarily occurred. \n",
    "- A use case for a warning log could be an unexpected number of rows, or previously unseen data types. \n",
    "- Error logs are used when an exception occurs that should halt the execution of a pipeline, such as when data has changed format, or is totally unavailable. \n",
    "- Properly created logs can save Data Engineers time when trying to discover why a pipeline failed, or why results have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "# Create different types of logs\n",
    "logging.warning(\"Unexpected number of rows detected.\")\n",
    "logging.error(\"{ke} arose in execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WARNING: Unexpected number of rows detected.\n",
    "ERROR: KeyError arose in execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Handling exceptions with try-except**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When building a data pipeline, it's common for errors to occur. \n",
    "- The best data pipelines handle common exceptions, and create logs to help debug. \n",
    "- One of the most basic ways to handle these exceptions is by using Python's built-in try-except logic. \n",
    "- This functionality allows for code to be run in the \"try\" block, and if an error occurs, rather than ending execution, code in the except block will be triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "# Execute some code here\n",
    "...\n",
    "except:\n",
    "# Logging about failures that occured\n",
    "# Logic to execute upon exception\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provides a way to execute code if errors occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Handling specific exceptions with try-except**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the specific exception in the except clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to filter by price_change\n",
    "    clean_stock_data = transform(raw_stock_data)\n",
    "    logging.info(\"Successfully filtered DataFrame by 'price_change'\")\n",
    "except KeyError as ke:\n",
    "    # Handle the error, create new column, transform\n",
    "    logging.warning(f\"{ke}: Cannot filter DataFrame by 'price_change'\")\n",
    "    raw_stock_data[\"price_change\"] = raw_stock_data[\"close\"] - raw_stock_data[\"open\"]\n",
    "    clean_stock_data = transform(raw_stock_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
