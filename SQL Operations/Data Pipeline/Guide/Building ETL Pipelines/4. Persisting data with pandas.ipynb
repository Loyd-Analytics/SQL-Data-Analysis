{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Persisting data in an ETL pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading data to a file is a great way to ensure that data consumers, including data scientists and analysts, have stable access to extracted and transformed data. \n",
    "- While typically most visible during the \"load\" portion of an ETL process, data persistence is a best practice that can, and should, happen at multiple stages in a data pipeline. \n",
    "- Persisting data to a file allows for a \"snapshot\" to be taken at various points throughout the pipeline. \n",
    "- This is especially useful when recovering from a failure, and is essential if data is hard to reacquire from a source system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data to a file:\n",
    "- Ensures data consumers have stable access to transformed data\n",
    "- Occurs as a final step in an ETL process, as well as between discrete steps\n",
    "- Captures a \"snapshot\" of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading data to CSV files using pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas makes it easy to write a DataFrame to a file, using the to_csv method. \n",
    "- In our example, our raw stock market data is loaded into a DataFrame, and a basic transformation is applied. \n",
    "- Then, the to_csv method is called on the DataFrame, with the desired file path for the DataFrame passed as an argument. \n",
    "- This DataFrame is then written to the relative path stock_data-dot-csv. \n",
    "- In addition to the file path, there are a few more arguments that customize how a DataFrame is to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".to_csv() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data extraction and transformation\n",
    "raw_data = pd.read_csv(\"raw_stock_data.csv\")\n",
    "stock_data = raw_data.loc[raw_data[\"open\"] > 100, [\"timestamps\", \"open\"]]\n",
    "\n",
    "# Load data to a .csv file\n",
    "stock_data.to_csv(\"stock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .to_csv called on the DataFrame\n",
    "- Writes DataFrame to path \"stock_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Customizing CSV file output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first is the header argument. This can take value True, False, or a list of strings to alias column names. \n",
    "- By default, this argument is True. Another argument that can be passed to the to_csv method is the index argument. \n",
    "- This too can take value True or False, defaulting to True. When True, the index column will be written to the file. \n",
    "- Typically, this is useful if an index column is meaningful to the dataset, such as when it stores a unique transaction ID. \n",
    "- Otherwise, it's best practice to set index to False. \n",
    "- The final argument that we'll dig into is the sep argument. sep takes a string, which is used to separate columns in the CSV file. \n",
    "- The default value is a comma, but a common alternative is the pipe character. \n",
    "- The to_csv method has counterparts that load data to different storage media, such as the to_parquet, and the to_json method. Later in the course, we'll explore the to_sql method, which writes a DataFrame to a SQL table. \n",
    "- In addition to these three methods, there are several others that can be used to load a DataFrame to a file or database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_csv(\"./stock_data.csv\", header=True)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes `True`, `False` or list of string values                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_csv(\"./stock_data.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes `True` or `False`\n",
    "- Determines whether `index` column is written to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_csv(\"./stock_data.csv\", sep=\"|\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes string value used to separate columns\n",
    "in the file\n",
    "- The | character is a common option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has counterparts:\n",
    "\n",
    "- `.to_parquet()`\n",
    "- `.to_json()`\n",
    "- `.to_sql()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensuring data persistence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we've written a DataFrame to a file, our next thought might be, \"how do we know that worked?\". \n",
    "- A great way to validate this is to check the file path, and ensure the file has been stored there. \n",
    "- This can be done with the help of the os module. \n",
    "- Here, we use the path-dot-exists function to check if a file exists at the path that's passed to the function. \n",
    "- In our example, we store the output of this function to a variable to inspect its value. \n",
    "- If the value of the variable is True, then the file path exists. \n",
    "- If False, an error may have occurred along the way, causing the DataFrame to not be persisted. \n",
    "- This is a great tool to use when validating the \"load\" step in a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the DataFrame correctly stored to the CSV file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os # Import the os module\n",
    "\n",
    "# Extract, transform and load data\n",
    "raw_data = pd.read_csv(\"raw_stock_data.csv\")\n",
    "stock_data = raw_data.loc[raw_data[\"open\"] > 100, [\"timestamps\", \"open\"]]\n",
    "stock_data.to_csv(\"stock_data.csv\")\n",
    "\n",
    "# Check that the path exists\n",
    "file_exists = os.path.exists(\"stock_data.csv\")\n",
    "print(file_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
