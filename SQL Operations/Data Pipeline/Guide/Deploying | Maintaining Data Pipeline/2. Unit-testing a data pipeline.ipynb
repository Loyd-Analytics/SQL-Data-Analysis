{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unit-testing a data pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unit tests are code that helps to test the functionality of other code and are commonly used in software engineering workflows. \n",
    "- Unit tests are the foundation for code validation, and can be used by data engineers to ensure components of a data pipeline work as expected. \n",
    "- Unit tests can also be written to validate data produced by a pipeline. In a typical data pipeline workflow, unit tests will be written and run before end-to-end validation is completed, to validate both the code and resulting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests:\n",
    "\n",
    "- Commonly used in software engineering workflows\n",
    "- Ensure code works as expected\n",
    "- Help to validate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **pytest for unit testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To build and run unit tests with Python, we'll be using a library called pytest. With pytest, unit tests are written as functions. \n",
    "- Typically, these function names start with \"test\", which allows pytest to automatically parse and run tests within a project. \n",
    "- In this example, we define a function test_transformed_data. This function asserts that the clean_stock_data object indeed takes type pd-dot-DataFrame. \n",
    "- When the command python dash-m pytest is executed, this test will be parsed and run. \n",
    "- If no AssertionErrors are raised, a success message will be output. \n",
    "- Let's take a closer look at the isinstance function, as well as the assert keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import extract, transform, load\n",
    "\n",
    "# Build a unit test, asserting the type of clean_stock_data\n",
    "def test_transformed_data():\n",
    "    raw_stock_data = extract(\"raw_stock_data.csv\")\n",
    "    clean_stock_data = transform(raw_stock_data)\n",
    "    assert isinstance(clean_stock_data, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "> python -m pytest\n",
    "\n",
    "test_transformed_data .                               [100%]\n",
    "============================= 1 passed in 1.17s =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **assert and isinstance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To check the object's type, we'll use the isinstance function. \n",
    "- isinstance takes two arguments: an object and a data type. \n",
    "- If the object matches the data type, the function returns True. \n",
    "- Otherwise, isinstance will return False. Here, \"ETL\" is assigned to the pipeline_type variable, and isinstance returns True when called, since pipeline_type takes the type string. \n",
    "- The assert keyword validates that a boolean expression is indeed True, and raises an AssertionError otherwise. \n",
    "- Here, we validate that pipeline_type indeed takes the value \"ETL\". \n",
    "- Since the statement evaluates to True, no error is raised. \n",
    "- When writing unit tests, we'll use assert and isinstance together to validate the type that objects take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_type = \"ETL\"\n",
    "\n",
    "# Check if pipeline_type is an instance of a str\n",
    "isinstance(pipeline_type, str)\n",
    "\n",
    "True\n",
    "\n",
    "# Assert that the pipeline does indeed take value \"ETL\"\n",
    "assert pipeline_type == \"ETL\"\n",
    "\n",
    "# Combine assert and isinstance\n",
    "assert isinstance(pipeline_type, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AssertionError**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, \"ETL\" is again assigned to the pipeline_type variable. \n",
    "- This time, we attempt to assert that this object is a float. \n",
    "- Since this is False, an AssertionError is raised, as shown here. \n",
    "- If this statement were placed within a unit test, the test would fail when run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_type = \"ETL\"\n",
    "# Create an AssertionError\n",
    "assert isinstance(pipeline_type, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 4, in <module>\n",
    "AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mocking data pipeline components with fixtures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pytest fixtures are functions that allow test data and objects to be shared across multiple tests. \n",
    "- They can be used to simplify test setup, and provide a common set of test data for multiple tests. \n",
    "- In this example, we create a fixture called clean_data, which returns a cleaned DataFrame. \n",
    "- The fixture is then passed to the test_transformed_data function. \n",
    "- When run, this unit test will be able to access the cleaned DataFrame created and returned by the clean_data fixture. \n",
    "- We'll explore fixtures more in the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def clean_data():\n",
    "  raw_stock_data = extract(\"raw_stock_data.csv\")\n",
    "  clean_stock_data = transform(raw_data)\n",
    "  return clean_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformed_data(clean_data):\n",
    "    assert isinstance(clean_data, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unit testing DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition to testing functions, we can also test data. \n",
    "- In this example, we'll test the clean_data DataFrame passed into the test as a fixture. \n",
    "- Using the dot-columns attribute, we assert that there are four columns in this DataFrame. \n",
    "- We can use other built-in tools, such as dot-min, to assert that all values in the open column take value greater than zero. \n",
    "- This can be taken one step further by validating the max value of this column with the dot-max method. \n",
    "- Running unit tests against data helps to confirm that data follows business rules and requirements, and can help to catch data quality issues before a pipeline is shipped to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformed_data(clean_data):\n",
    "    # Include other assert statements here\n",
    "    ...\n",
    "\n",
    "    # Check number of columns\n",
    "    assert len(clean_data.columns) == 4\n",
    "\n",
    "    # Check the lower bound of a column\n",
    "    assert clean_data[\"open\"].min() >= 0\n",
    "    \n",
    "    # Check the range of a column by chaining statements with \"and\"\n",
    "    assert clean_data[\"open\"].min() >= 0 and clean_data[\"open\"].max() <= 1000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
