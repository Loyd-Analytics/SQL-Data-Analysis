{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data pipeline architecture patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When building data pipelines, it's best to separate the files where functions are being defined from where they are being run.\n",
    "\n",
    "- In this exercise, you'll practice importing components of a pipeline into memory before using these functions to run the pipeline end-to-end. The project takes the following format, where pipeline_utils stores the extract(), transform(), and load() functions that will be used run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> ls\n",
    " etl_pipeline.py\n",
    " pipeline_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "- Import the extract, transform, and load functions from the pipeline_utils module.\n",
    "- Use the functions imported to run the data pipeline end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the extract, transform, and load functions from pipeline_utils\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "# Run the pipeline end to end by extracting, transforming and loading the data\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
