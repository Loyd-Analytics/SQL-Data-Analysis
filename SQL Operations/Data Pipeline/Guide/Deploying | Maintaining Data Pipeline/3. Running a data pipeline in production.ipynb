{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data pipeline architecture patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Several techniques exist to run a data pipeline in a production-like setting. \n",
    "- One of the most common is by executing a script that triggers the extract, transform, and load logic that forms the pipeline. \n",
    "- On the left, both the function definitions and the execution of the pipeline exist in a single file. \n",
    "- While this is an easy way to define and run a pipeline, it's not always the best. \n",
    "- Too much code in a single file can cause confusion when debugging or sharing your code. \n",
    "- A better way to architect a pipeline involves storing function definitions in a separate file from the execution logic. \n",
    "- Then, these functions can be imported and called as needed. On the right, the extract, transform, and load functions are stored in the pipeline_utils-dot-py file. \n",
    "- When needed for execution, they are imported and called. \n",
    "- This architecture pattern helps to separate execution details from the definitions of the extract, transform, and load logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ETL function\n",
    "...\n",
    "def load(clean_data):\n",
    "...\n",
    "\n",
    "# Run the data pipeline\n",
    "raw_stock_data = extract(\"raw_stock_data.csv\")\n",
    "clean_stock_data = transform(raw_stock_data)\n",
    "load(clean_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "> ls\n",
    "etl_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extract, transform, and load functions\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "# Run the data pipeline\n",
    "raw_stock_data = extract(\"raw_stock_data.csv\")\n",
    "clean_stock_data = transform(raw_stock_data)\n",
    "load(clean_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "> ls\n",
    "etl_pipeline.py\n",
    "pipeline_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running a data pipeline end-to-end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the architecture we have just discussed, we can combine all of the tools we've learned so far to run the pipeline end-to-end. \n",
    "- First, we'll import the logging module, as well as the extract, transform, and load functions from the pipeline_utils-dot-py file. \n",
    "- After the logger is configured, we can execute our ETL logic inside a try-except block. \n",
    "- This, coupled with our logging statements creates a basic monitoring and alerting solution that allows for a Data Engineer to track the execution of a pipeline. \n",
    "- When placed within a dot-py script, this pipeline can be executed, and will extract, transform, and load data in a production environment. \n",
    "- However, there is some added functionality that most Data Engineers look for when deploying a pipeline to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "    # Extract, transform, and load data\n",
    "    raw_stock_data = extract(\"raw_stock_data.csv\")\n",
    "    clean_stock_data = transform(raw_stock_data)\n",
    "    load(clean_stock_data)\n",
    "    \n",
    "    logging.info(\"Successfully extracted, transformed and loaded data.\")  # Log success message\n",
    "\n",
    "# Handle exceptions, log messages\n",
    "except Exception as e:\n",
    "    logging.error(f\"Pipeline failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Orchestrating data pipelines in production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When choosing how to deploy a data pipeline, Data Engineers look for the ability to automatically run a pipeline on a schedule, with flexible resources available at run time. \n",
    "- In addition to this, it's important that a pipeline retry on failure, and alert when an error occurs. \n",
    "- While we've been able to implement some of this logic using homegrown tooling, most Data Engineers will reach for an orchestration, or ETL, tool to help deliver a production-grade pipeline. \n",
    "- Orchestration tools deliver additional features to a pipeline's execution, including scheduling, resource scaling, and more robust error handling. \n",
    "- The most commonly-used orchestration tool is Apache Airflow. \n",
    "- A little more than forty percent of all data pipelines are implemented using this tool. \n",
    "- In terms of market share, Airflow is by far and away the most popular tool for building, deploying, and monitoring data pipelines. \n",
    "- In addition to being open source, it offers a wide range of features and plug-ins. \n",
    "- Outside of Airflow, tools such as Prefect and Dagster are also used for orchestration. \n",
    "- Despite the wide range of features that third-party orchestration tools bring to the table, many pipelines are still built without the help of a third-party tool, and use homegrown or custom-built solutions. \n",
    "- Choosing an orchestration tool is an important decision, and if the right tool is chosen, it can help to dramatically expedite pipeline development."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
